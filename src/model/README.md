<div align="center">

# ğŸ“ Pariksha Guardian Nexus - AI Model Documentation

### *Intelligent Exam Proctoring with Computer Vision & Deep Learning*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://tensorflow.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green.svg)](https://opencv.org/)
[![Flask](https://img.shields.io/badge/Flask-2.x-black.svg)](https://flask.palletsprojects.com/)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture & Algorithms](#-architecture--algorithms)
- [Project Structure](#-project-structure)
- [API Endpoints](#-api-endpoints)
- [Dependencies](#-dependencies)
- [Quick Start](#-quick-start)
- [Performance Metrics](#-performance-metrics)
- [Proctoring Logic](#-proctoring-logic)
- [Future Enhancements](#-future-enhancements)

---

## ğŸ¯ Overview

The **Pariksha Guardian Nexus** implements a sophisticated **multi-model AI architecture** for real-time exam proctoring, combining state-of-the-art computer vision algorithms to ensure academic integrity.

### âœ¨ Core Capabilities

<table>
<tr>
<td width="50%">

ğŸ” **Face Detection & Tracking**
- Real-time face localization
- High accuracy detection

</td>
<td width="50%">

ğŸ“ **68-Point Facial Landmarks**
- Precise feature extraction
- Sub-pixel accuracy

</td>
</tr>
<tr>
<td width="50%">

ğŸ§­ **Head Pose Estimation**
- 3D orientation tracking
- Gaze direction analysis

</td>
<td width="50%">

ğŸ‘¥ **Multiple Person Detection**
- Unauthorized presence detection
- COCO dataset trained

</td>
</tr>
</table>

---

## ğŸ§  Architecture & Algorithms

### 1ï¸âƒ£ Face Detection Module

<details>
<summary><b>ğŸ“¸ Click to expand: SSD-based Face Detector</b></summary>

#### **Algorithm**: Single Shot MultiBox Detector (SSD) with ResNet-10 backbone

**ğŸ”§ Model Specifications:**
| Parameter | Value |
|-----------|-------|
| Framework | OpenCV DNN (Caffe) |
| Input Size | 300Ã—300 RGB |
| Model File | `res10_300x300_ssd_iter_140000.caffemodel` |
| Config File | `deploy.prototxt` |
| Confidence Threshold | 0.5 (default) |

**ğŸ”„ Processing Pipeline:**

```mermaid
graph LR
    A[Input Image] --> B[Blob Conversion<br/>300Ã—300]
    B --> C[Mean Subtraction<br/>[104, 177, 123]]
    C --> D[SSD Forward Pass]
    D --> E[Confidence Filtering]
    E --> F[Bounding Boxes]
```

**âš¡ Key Features:**
- Mean subtraction: `[104.0, 177.0, 123.0]`
- Returns bounding boxes with confidence scores
- Real-time processing capability

</details>

---

### 2ï¸âƒ£ Facial Landmark Detection

<details>
<summary><b>ğŸ¯ Click to expand: CNN-based Landmark Detector</b></summary>

#### **Algorithm**: Convolutional Neural Network for Facial Landmark Regression

**ğŸ”§ Model Specifications:**
| Parameter | Value |
|-----------|-------|
| Framework | TensorFlow/Keras |
| Model Path | `assets/pose_model` |
| Input Size | 128Ã—128 RGB |
| Output | 68 (x, y) coordinates |

**ğŸ¨ Landmark Distribution:**

```
    ğŸ‘ï¸ Eyes: Points 36-47 (12 points)
    ğŸ‘ƒ Nose: Points 27-35 (9 points)
    ğŸ‘„ Mouth: Points 48-67 (20 points)
    ğŸ­ Jawline: Points 0-16 (17 points)
    ğŸ‘‚ Eyebrows: Points 17-26 (10 points)
```

**ğŸ”„ Processing Pipeline:**

1. ğŸ” Extract face region using `FaceDetector`
2. ğŸ“ Apply geometric transformations:
   - â¬‡ï¸ Box offset (12% downward)
   - â¬› Square box expansion
   - âœ… Boundary validation
3. ğŸ“ Resize to 128Ã—128 pixels
4. ğŸ§  CNN inference for 68 landmarks
5. ğŸ”„ Transform coordinates to original space

</details>

---

### 3ï¸âƒ£ Head Pose Estimation

<details>
<summary><b>ğŸ§­ Click to expand: PnP-based Pose Estimator</b></summary>

#### **Algorithm**: Perspective-n-Point (PnP) with 3D-2D Correspondence

**ğŸ”§ Model Specifications:**
| Parameter | Value |
|-----------|-------|
| Method | `cv2.solvePnP` (iterative) |
| 3D Model | 68-point facial model |
| Model File | `assets/model.txt` |
| Camera Model | Pinhole camera |

**ğŸ¯ Pose Classification Thresholds:**

| Pose Direction | Angle Range | Status |
|---------------|-------------|---------|
| ğŸ‘† Looking Up | Pitch < -15Â° | âš ï¸ Warning |
| ğŸ‘‡ Looking Down | Pitch > 15Â° | âš ï¸ Warning |
| ğŸ‘ˆ Looking Left | Yaw < -20Â° | âš ï¸ Warning |
| ğŸ‘‰ Looking Right | Yaw > 20Â° | âš ï¸ Warning |
| ğŸ‘ï¸ Looking Straight | \|pitch\| < 10Â° AND \|yaw\| < 10Â° | âœ… Normal |

**ğŸ”„ Processing Pipeline:**

```mermaid
graph TD
    A[68 2D Landmarks] --> B[Load 3D Model]
    B --> C[Solve PnP Problem]
    C --> D[Rotation Vector<br/>3Ã—1 matrix]
    C --> E[Translation Vector<br/>3Ã—1 matrix]
    D --> F[Rodrigues Transform]
    F --> G[Euler Angles<br/>Pitch, Yaw, Roll]
    G --> H[cv2.RQDecomp3x3]
    H --> I[Pose Classification]
```

**ğŸ“Š Output Vectors:**
- ğŸ”„ **Rotation Vector**: 3Ã—1 axis-angle representation
- ğŸ“ **Translation Vector**: 3Ã—1 camera position
- âš™ï¸ Uses previous frame for temporal stability

</details>

---

### 4ï¸âƒ£ Multiple Person Detection

<details>
<summary><b>ğŸ‘¥ Click to expand: EfficientDet Detector</b></summary>

#### **Algorithm**: EfficientDet D0 Object Detection

**ğŸ”§ Model Specifications:**
| Parameter | Value |
|-----------|-------|
| Framework | TensorFlow Hub |
| Model | `tensorflow/efficientdet/d0/1` |
| Dataset | COCO (80 classes) |
| Person Class ID | 1 |
| Confidence Threshold | 0.5 |

**ğŸ”„ Detection Process:**

1. ğŸ“¥ Load pre-trained EfficientDet D0
2. ğŸ” Full image object detection
3. ğŸ¯ Filter for "person" class (ID=1)
4. âœ‚ï¸ Apply confidence threshold
5. ğŸ”¢ Count detected persons

</details>

---

## ğŸ“ Project Structure

```
model/
â”œâ”€â”€ ğŸ“‚ assets/                          # Model files & configurations
â”‚   â”œâ”€â”€ ğŸ“„ deploy.prototxt              # SSD architecture
â”‚   â”œâ”€â”€ ğŸ“¦ res10_300x300_ssd_iter_140000.caffemodel
â”‚   â”œâ”€â”€ ğŸ“‚ pose_model/                  # CNN landmark model
â”‚   â””â”€â”€ ğŸ“„ model.txt                    # 3D facial coordinates
â”œâ”€â”€ ğŸ–¼ï¸ images/                          # Documentation images
â”œâ”€â”€ ğŸ mark_detector.py                 # Face & landmark detection
â”œâ”€â”€ ğŸ pose_estimator.py                # Head pose estimation
â””â”€â”€ ğŸ app.py                           # Flask API server
```

---

## ğŸš€ API Endpoints

### REST API Server (Flask)

<details>
<summary><b>ğŸ”µ POST `/predict_pose` - Basic Pose Detection</b></summary>

**Purpose**: Basic head pose detection and classification

**ğŸ“¥ Request:**
```json
{
  "img": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
}
```

**ğŸ“¤ Response:**
```json
{
  "message": "face found",
  "pose": {
    "rotation_vector": [0.12, -0.05, 0.03],
    "translation_vector": [-15.2, -10.5, -2050.1]
  },
  "head_pose": {
    "looking_up": false,
    "looking_down": false,
    "looking_left": false,
    "looking_right": false,
    "looking_straight": true,
    "pitch": -5.2,
    "yaw": 3.1,
    "roll": 1.8
  },
  "warnings": []
}
```

</details>

<details>
<summary><b>ğŸŸ¢ POST `/predict_pose_detailed` - Enhanced Detection with Visualization</b></summary>

**Purpose**: Pose detection with annotated image output

**âœ¨ Additional Features:**
- ğŸ“ Visual annotations on face
- ğŸ¯ Facial landmarks overlay
- ğŸ“Š Pose angle text display
- âš ï¸ Warning indicators
- ğŸ–¼ï¸ Returns annotated base64 image

**ğŸ“¤ Response:**
```json
{
  "message": "face found",
  "pose": { ... },
  "head_pose": { ... },
  "annotated_image": "data:image/jpeg;base64,...",
  "warnings": ["Student is looking left"]
}
```

</details>

<details>
<summary><b>ğŸŸ£ POST `/predict_people` - Multiple Person Detection</b></summary>

**Purpose**: Detect unauthorized persons in frame

**ğŸ“¤ Response:**
```json
{
  "people": 2,
  "image": "image"
}
```

</details>

<details>
<summary><b>ğŸŸ¡ POST `/save_img` - Image Audit Storage</b></summary>

**Purpose**: Save captured images for compliance audit

</details>

<details>
<summary><b>ğŸŸ¢ GET `/health` - Health Check</b></summary>

**Purpose**: System status verification

**ğŸ“¤ Response:**
```json
{
  "status": "healthy",
  "model": "loaded",
  "endpoints": ["/predict_pose", "/predict_pose_detailed", ...]
}
```

</details>

---

## ğŸ“¦ Dependencies

### Core Technology Stack

<table>
<tr>
<td align="center" width="25%">
<img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" />
<br><b>Deep Learning</b>
<br>Framework for CNN models
</td>
<td align="center" width="25%">
<img src="https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white" />
<br><b>Computer Vision</b>
<br>DNN module, PnP solver
</td>
<td align="center" width="25%">
<img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white" />
<br><b>Numerical Computing</b>
<br>Array operations
</td>
<td align="center" width="25%">
<img src="https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white" />
<br><b>REST API</b>
<br>Server framework
</td>
</tr>
</table>

### ğŸ“‹ Required Model Files

| File | Purpose |
|------|---------|
| `deploy.prototxt` | SSD architecture definition |
| `res10_300x300_ssd_iter_140000.caffemodel` | Pre-trained SSD weights |
| `assets/pose_model/` | Keras landmark detection model |
| `assets/model.txt` | 3D facial landmark coordinates |

---

## âš¡ Quick Start

### ğŸ”§ Initialize Models

```python
from mark_detector import MarkDetector, FaceDetector
from pose_estimator import PoseEstimator

# Initialize detectors
mark_detector = MarkDetector()
pose_estimator = PoseEstimator(img_size=(height, width))

# Detect face
facebox = mark_detector.extract_cnn_facebox(image)

# Detect landmarks
marks = mark_detector.detect_marks(face_image)

# Estimate pose
rotation_vector, translation_vector = pose_estimator.solve_pose_by_68_points(marks)
```

### ğŸš€ Run Flask Server

```bash
python app.py
# ğŸŒ Server runs on http://0.0.0.0:8080
```

---

## ğŸ“Š Performance Metrics

<table>
<tr>
<th>Algorithm</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Notes</th>
</tr>
<tr>
<td><b>ğŸ” Face Detection</b></td>
<td>âš¡ 30-50ms</td>
<td>ğŸ¯ High (0.9 threshold)</td>
<td>âš ï¸ Struggles with extreme lighting</td>
</tr>
<tr>
<td><b>ğŸ“ Landmark Detection</b></td>
<td>âš¡ 10-20ms</td>
<td>ğŸ¯ Sub-pixel precision</td>
<td>âœ… Requires good face detection</td>
</tr>
<tr>
<td><b>ğŸ§­ Pose Estimation</b></td>
<td>âš¡ 5-10ms</td>
<td>ğŸ¯ Â±5-10Â° error</td>
<td>ğŸ”„ Temporal smoothing enabled</td>
</tr>
<tr>
<td><b>ğŸ‘¥ Person Detection</b></td>
<td>âš¡ 100-200ms</td>
<td>ğŸ¯ mAP ~34%</td>
<td>ğŸ“Š COCO dataset performance</td>
</tr>
</table>

---

## ğŸš¨ Proctoring Logic

### Suspicious Behavior Detection

<table>
<tr>
<td width="50%">

#### âš ï¸ Warning Triggers

| Behavior | Detection Method |
|----------|------------------|
| âŒ **No Face** | Face detection fails |
| ğŸ‘† **Looking Up** | Ceiling reference check |
| ğŸ‘‡ **Looking Down** | Notes/phone detection |
| ğŸ‘ˆğŸ‘‰ **Looking Sideways** | Collaboration detection |
| ğŸ‘¥ **Multiple People** | Unauthorized assistance |

</td>
<td width="50%">

#### âœ… Normal Behavior

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚
â”‚    ğŸ‘ï¸ Straight     â”‚
â”‚   Gaze Forward      â”‚
â”‚                     â”‚
â”‚  |Pitch| < 10Â°      â”‚
â”‚  |Yaw| < 10Â°        â”‚
â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
</tr>
</table>

---

## ğŸ”® Future Enhancements

<div align="center">

### ğŸ¯ Roadmap

```mermaid
graph LR
    A[Current System] --> B[Eye Gaze Tracking]
    A --> C[Audio Analysis]
    A --> D[Object Detection]
    A --> E[Behavior Patterns]
    A --> F[Emotion Recognition]
    
    B --> G[Enhanced Monitoring]
    C --> G
    D --> G
    E --> G
    F --> G
```

</div>

| Feature | Description | Status |
|---------|-------------|--------|
| ğŸ‘ï¸ **Eye Gaze Tracking** | Precise attention monitoring | ğŸ”œ Planned |
| ğŸ”Š **Audio Analysis** | Suspicious sound detection | ğŸ”œ Planned |
| ğŸ“± **Object Detection** | Phone, books, materials | ğŸ”œ Planned |
| ğŸ“ˆ **Temporal Analysis** | Behavior pattern tracking | ğŸ”œ Planned |
| ğŸ˜Š **Emotion Recognition** | Stress detection | ğŸ”œ Planned |

---

<div align="center">

### ğŸ’¡ Built with â¤ï¸ for Academic Integrity

**Made with cutting-edge AI/ML technologies**

[![GitHub](https://img.shields.io/badge/GitHub-Repository-black?style=for-the-badge&logo=github)](https://github.com)
[![Documentation](https://img.shields.io/badge/Documentation-Complete-green?style=for-the-badge&logo=readthedocs)](.)
